{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages / libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def load_data(csv_file):\n",
    "    dataset = pd.read_csv(csv_file)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This function should split a data set into a training set and hold-out test set\n",
    "def split_data(dataset, testset_Ratio):\n",
    "    X = dataset.drop(columns = 'Grade')\n",
    "    y = dataset[['Grade']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testset_Ratio, random_state = 0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(X_train, y_train):\n",
    "    \n",
    "    y_train_values = y_train['Grade'].values\n",
    "    train_set = X_train.join(y_train)\n",
    "    \n",
    "#First, calculate the prior probabilities    \n",
    "    \n",
    "    AA = 0\n",
    "    A = 0\n",
    "    B = 0\n",
    "    C = 0 \n",
    "    D = 0 \n",
    "    F = 0\n",
    "\n",
    "\n",
    "    for grade in y_train_values:\n",
    "        if grade == 'A+':\n",
    "            AA += 1\n",
    "        elif grade == 'A':\n",
    "            A += 1\n",
    "        elif grade == 'B':\n",
    "            B += 1\n",
    "        elif grade == 'C':\n",
    "            C += 1\n",
    "        elif grade == 'D':\n",
    "            D += 1\n",
    "        elif grade == 'F':\n",
    "            F += 1\n",
    "\n",
    "\n",
    "    count_list = [AA,A,B,C,D,F]\n",
    "    prior_list =[]\n",
    "\n",
    "    for grade_count in count_list:\n",
    "        prior_list.append(grade_count/len(y_train))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#helper function to iterate through the features and calculate count the number of occurences of a categorical value \n",
    "#given a class variable    \n",
    "\n",
    "    def count(data,colname,value,target):\n",
    "        condition = (data[colname] == value) & (data['Grade'] == target)\n",
    "        return len(data[condition])    \n",
    "    \n",
    "    \n",
    "\n",
    "#Determine the probabilities P(X|Y) of desired attribute (eg. sex, age, location) conditioned on a given grade (eg. A+,A...)    \n",
    "    \n",
    "    probabilities = {'A+':{},'A':{},'B':{},'C':{},'D':{},'F':{}}\n",
    "    #set up a dictionary containing grade and their corresponding prior probabilities\n",
    "    grade_dict = {'A+':count_list[0], 'A':count_list[1], 'B':count_list[2], \n",
    "                  'C':count_list[3], 'D':count_list[4], 'F':count_list[5],}\n",
    "\n",
    "    for grade in probabilities:\n",
    "\n",
    "        for col in X_train.columns:\n",
    "            probabilities[grade][col] = {}\n",
    "            feature_values = X_train[col].values\n",
    "            check_duplicate = []\n",
    "\n",
    "            for category in feature_values:\n",
    "    # if statement in place to avoid redundant calculations\n",
    "                if category not in check_duplicate:\n",
    "                    check_duplicate.append(category)\n",
    "                    count_ct = count(train_set, col, category, grade)\n",
    "                    probabilities[grade][col][category] = count_ct/grade_dict[grade]    \n",
    "    \n",
    "    return probabilities, prior_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "\n",
    "def predict(X_test, prior_list, probabilities):\n",
    "    \n",
    "    predicted = []\n",
    "\n",
    "#Find posterior probability P(Y|X1,X2,X3) = P(Y)P(X1|Y)P(X2|Y)....\n",
    "\n",
    "\n",
    "    for row in range(len(X_test)):\n",
    "        posterior_probabilities = {'A+':prior_list[0], 'A':prior_list[1], 'B':prior_list[2], 'C':prior_list[3], 'D':prior_list[4], 'F':prior_list[5]}\n",
    "        for feature in X_test.columns:\n",
    "            value = X_test.iloc[row][feature]\n",
    "            for grade in posterior_probabilities:\n",
    "                posterior_probabilities[grade] *= probabilities[grade][feature][value] #P(Y = grade)P(feature = value |Y = grade)   \n",
    "\n",
    "#Predict the class after traversing through all features in a row by selecting the largest probability\n",
    "        predicted.append(max(posterior_probabilities, key=posterior_probabilities.get))  \n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions in terms of accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate(y_test, predicted):\n",
    "    return metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional part for Q3b using Leave one out validation\n",
    "\n",
    "#function to shuffle rows in dataframe to ensure our LOO validation works correctly\n",
    "def shuffle(df):\n",
    "    return df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "#helper function to iterate through the features and calculate count the number of occurences of a categorical value \n",
    "#given a class variable    \n",
    "\n",
    "def count(data,colname,value,target):\n",
    "    condition = (data[colname] == value) & (data['Grade'] == target)\n",
    "    return len(data[condition])    \n",
    "    \n",
    "\n",
    "#leave one out evaluation \n",
    "'''Function takes in a dataset, and perform LOO validation, and then evaluates the accuracy of LOO method as output'''\n",
    "def loo(dataset):\n",
    "    accuracy_list = []\n",
    "    for r in range(len(dataset)):\n",
    "    #train test split\n",
    "    #-----------------\n",
    "        train_set2 = dataset.drop(dataset.index[r]) #drop a row by row number from dataframe\n",
    "        X_train2 = train_set2.drop(columns = 'Grade')\n",
    "        y_train2 = train_set2[['Grade']]\n",
    "\n",
    "        test_set2 = dataset.iloc[r] #testing set consisting of one instance/observation (Leave one out)\n",
    "        X_test2 = test_set2.drop('Grade')\n",
    "        y_test2 = test_set2[['Grade']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Training stage\n",
    "    #---------------\n",
    "    #Calculate the prior probability of grades for each class label \n",
    "        AA = 0\n",
    "        A = 0\n",
    "        B = 0\n",
    "        C = 0 \n",
    "        D = 0 \n",
    "        F = 0\n",
    "\n",
    "        #determine the count of grades\n",
    "\n",
    "        for grade in y_train2.values:\n",
    "            if grade == 'A+':\n",
    "                AA += 1\n",
    "            elif grade == 'A':\n",
    "                A += 1\n",
    "            elif grade == 'B':\n",
    "                B += 1\n",
    "            elif grade == 'C':\n",
    "                C += 1\n",
    "            elif grade == 'D':\n",
    "                D += 1\n",
    "            elif grade == 'F':\n",
    "                F += 1\n",
    "\n",
    "    #determine the marginal probability of the grades\n",
    "\n",
    "        count_list2 = [AA,A,B,C,D,F]\n",
    "        prior_list2 =[]\n",
    "\n",
    "        for grade_count in count_list2:\n",
    "            prior_list2.append(grade_count/len(y_train2))    \n",
    "\n",
    "\n",
    "\n",
    "        probabilities2 = {'A+':{},'A':{},'B':{},'C':{},'D':{},'F':{}}\n",
    "        #set up a dictionary containing grade and their corresponding prior probabilities\n",
    "        grade_dict2 = {'A+':count_list2[0], 'A':count_list2[1], 'B':count_list2[2], \n",
    "                      'C':count_list2[3], 'D':count_list2[4], 'F':count_list2[5],}\n",
    "\n",
    "        for grade in probabilities2:\n",
    "\n",
    "            for col in X_train2.columns:\n",
    "                probabilities2[grade][col] = {}\n",
    "                feature_values2 = X_train2[col].values\n",
    "                check_duplicate2 = []\n",
    "\n",
    "                for category in feature_values2:\n",
    "        # if statement in place to avoid redundant calculations\n",
    "                    if category not in check_duplicate2:\n",
    "                        check_duplicate2.append(category)\n",
    "                        count_ct2 = count(train_set2, col, category, grade)                  \n",
    "                        probabilities2[grade][col][category] = count_ct2/grade_dict2[grade]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #TRAINING STAGE COMPLETE\n",
    "    #---------------------------\n",
    "\n",
    "\n",
    "\n",
    "    #Testing stage\n",
    "    #---------------\n",
    "    \n",
    "    #Find posterior probability P(Y|X1,X2,X3) = P(Y)P(X1|Y)P(X2|Y)....\n",
    "        predicted2 = 0\n",
    "\n",
    "\n",
    "\n",
    "        posterior_probabilities2 = {'A+':prior_list2[0], 'A':prior_list2[1], 'B':prior_list2[2], 'C':prior_list2[3], 'D':prior_list2[4], 'F':prior_list2[5]}\n",
    "\n",
    "        for feature in X_train2.columns:\n",
    "            value = X_test2[feature]\n",
    "            for grade in posterior_probabilities2:\n",
    "                posterior_probabilities2[grade] *= probabilities2[grade][feature][value] #P(Y = grade)P(feature = value |Y = grade)   \n",
    "\n",
    "        #Predict the class after traversing through all features in a row by selecting the largest probability\n",
    "        predicted2 = max(posterior_probabilities2, key=posterior_probabilities2.get)\n",
    "\n",
    "        if predicted2 == y_test2['Grade']:\n",
    "            accuracy_list.append(1)\n",
    "        else:\n",
    "            accuracy_list.append(0)\n",
    "        print(\"number of rows left to evaluate: \", len(dataset)-len(accuracy_list))\n",
    "\n",
    "        \n",
    "    #TRAINING STAGE COMPLETE\n",
    "    #---------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Evaluation stage\n",
    "    #----------------------\n",
    "\n",
    "    average_accuracy = sum(accuracy_list)/len(dataset)\n",
    "    print('Accuracy for LOO Method: ', average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Discussion\n",
    "\n",
    "- Naïve Bayes assumes conditional independence, which states that features are independent of each other given the class. \n",
    "- The resulting model is simple, easy to build and can compute predictions in a short amount of time. \n",
    "- It performs well in small data sets, and scales well with additional features, classes and data sizes. \n",
    "- It is necessary to make our computations cheap and to make predictions in real time. \n",
    "- Without the underlying assumption, we will need to account for covariance between variables, \n",
    "- which requires many intensive calculations such as matric inverses determinants and thus making the training process computationally expensive. \n",
    "- However, in real life features are not completely independent. \n",
    "- For example, the number of school absences can be dependent on the health status of students; quality of family relationships may be dependent on parents’ cohabitation status and the students’ guardian. \n",
    "- Failure to account for the dependencies makes Naïve Bayes a bad estimator. \n",
    "- Due to the assumption of independence, if there no occurrences of a class label and a feature value together, \n",
    "- or a categorical variable has a category that was not observed in training process, \n",
    "- then the likelihood P(feature | class) estimated will be zero, and our model will be unable to make a prediction. \n",
    "- In summary, the assumption of independence makes Naïve Bayes a fast but inaccurate model to make predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using holdout validation:  0.3230769230769231\n"
     ]
    }
   ],
   "source": [
    "#Q1b\n",
    "#-----------------\n",
    "dataset = load_data(r\"C:\\Users\\cyeeh\\OneDrive\\Masters\\2020 Sem 1\\Intro to Machine Learning\\Assignment 1\\student.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(dataset, 0.2)\n",
    "probabilities, prior_list = train(X_train, y_train)\n",
    "predictions = predict(X_test, prior_list, probabilities)\n",
    "accuracy = evaluate(y_test, predictions)\n",
    "print('Accuracy using holdout validation: ', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>...</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>low</td>\n",
       "      <td>none</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>reputation</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>none</td>\n",
       "      <td>A+</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>high</td>\n",
       "      <td>mid</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>four_to_six</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>one_to_three</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>reputation</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>more_than_ten</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>high</td>\n",
       "      <td>mid</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>A+</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>at_home</td>\n",
       "      <td>at_home</td>\n",
       "      <td>course</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>more_than_ten</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>high</td>\n",
       "      <td>mid</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>reputation</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>reputation</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>four_to_six</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>other</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "375     GP   M       U     GT3       T   low  none     other     other   \n",
       "306     GP   M       U     GT3       T  high   mid     other     other   \n",
       "625     GP   M       U     GT3       T   mid   mid     other     other   \n",
       "480     GP   F       U     GT3       T   low   low   at_home     other   \n",
       "303     GP   M       U     LE3       T   mid   mid     other     other   \n",
       "..     ...  ..     ...     ...     ...   ...   ...       ...       ...   \n",
       "585     MS   F       U     GT3       T  high   mid     other     other   \n",
       "419     MS   M       U     GT3       T   mid   mid   at_home   at_home   \n",
       "236     GP   F       U     GT3       A  high   mid  services  services   \n",
       "641     GP   F       R     GT3       T   mid   mid  services     other   \n",
       "558     MS   F       R     LE3       T   mid   mid     other  services   \n",
       "\n",
       "         reason  ... romantic famrel freetime goout Dalc Walc health  \\\n",
       "375  reputation  ...      yes      4        3     2    1    1      3   \n",
       "306      course  ...      yes      5        2     3    1    1      2   \n",
       "625        home  ...       no      5        3     2    1    2      5   \n",
       "480  reputation  ...      yes      4        3     4    1    1      5   \n",
       "303      course  ...       no      4        5     5    2    4      5   \n",
       "..          ...  ...      ...    ...      ...   ...  ...  ...    ...   \n",
       "585      course  ...       no      4        3     3    1    2      4   \n",
       "419      course  ...       no      4        4     5    1    3      3   \n",
       "236  reputation  ...       no      4        3     2    1    1      1   \n",
       "641  reputation  ...       no      4        1     2    1    1      2   \n",
       "558      course  ...      yes      5        4     3    1    1      1   \n",
       "\n",
       "          absences Grade Predicted  \n",
       "375           none    A+         C  \n",
       "306    four_to_six     C         B  \n",
       "625   one_to_three     F         C  \n",
       "480  more_than_ten     C         B  \n",
       "303           none     C         D  \n",
       "..             ...   ...       ...  \n",
       "585           none    A+         C  \n",
       "419  more_than_ten     F         D  \n",
       "236           none     B         A  \n",
       "641    four_to_six     D         C  \n",
       "558           none     C         A  \n",
       "\n",
       "[88 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = X_test.join(y_test)\n",
    "merge_predict_actual = test_set.assign(Predicted = predictions)\n",
    "\n",
    "#Selected some incorrect predictions to compare\n",
    "selected_incorrect = merge_predict_actual[merge_predict_actual['Grade'] != merge_predict_actual['Predicted']]\n",
    "selected_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- The classifier achieves an accuracy of 32%. \n",
    "- Looking at instances where classifier correctly predicted a D or F final grade, a majority of the students’ parents possess low to medium level of education, and almost none at high. \n",
    "- Most of them seems to spend low to medium hours studying and have their mothers as the guardian. \n",
    "- There appears to be a lack of A+ grade prediction, which could be due to scarcity of A+ label in dataset. \n",
    "- For students with a correct grade prediction of A or B, the parents’ education seems to range mostly from medium to high, and their study hours are mostly medium to high. \n",
    "- This could flag parents’ education as a good indicator in predicting grades. \n",
    "- This is further confirmed when we look at the incorrect predictions, with students who have highly educated parents receiving high grade prediction despite scoring poor grade. \n",
    "- The students address also seem to play an important role in predicting grades, with students coming from urban background getting a good grade prediction regardless of their actual grade. \n",
    "- In both correct and incorrect predictions, it is observed that rural students are predicted a lower grade. \n",
    "- The sex of the student doesn’t seem to affect grades, and so does travel time and family size. \n",
    "- The model seems to view students from GS school more favourably as they receive an optimistically incorrect prediction, with many C students predicted A and even a case of F predicted B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative strategies\n",
    "\n",
    "- To further improve our evaluation, a  leave one out (LOO) strategy is used\n",
    "- It is an extreme case of k-fold cross validation strategy, with k = 649 which is the number of instances in this assignment.\n",
    "- The dataset is first separated into 649 partitions, where one partition is assigned as the testing set while the remainder is used to train the model. \n",
    "- This process is repeated until each unique partition has been used as the testing set. \n",
    "- The evaluation metric, in this case, the accuracy of the LOO model is calculated by averaging across the accuracy of each fold. \n",
    "\n",
    "\n",
    "- In comparison, the holdout method splits our dataset into training and testing set once, usually at a ratio of 80% to 20%. \n",
    "- The training set is used to train our model, while the testing set is used to measure the performance of our model on unseen data. \n",
    "- This results in a smaller training set which, in turn, introduces a bias and variance to our estimated error as the training data might not be an accurate representation of the underlying distribution of the dataset. \n",
    "- Thus, the estimates produced by the holdout method is highly dependent on how the train-test split is done. \n",
    "- To that end, the LOO method is highly preferred over the hold-out strategy as it allows evaluation to be done across multiple train-test splits. \n",
    "- It minimises the bias of our performance estimates because we can maximise our training data. \n",
    "- The size of the training data using the LOO method is only one instance smaller than the dataset, making our performance estimates approximately unbiased, as the training set would have a similar distribution to our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of LOO strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows left to evaluate:  648\n",
      "number of rows left to evaluate:  647\n",
      "number of rows left to evaluate:  646\n",
      "number of rows left to evaluate:  645\n",
      "number of rows left to evaluate:  644\n",
      "number of rows left to evaluate:  643\n",
      "number of rows left to evaluate:  642\n",
      "number of rows left to evaluate:  641\n",
      "number of rows left to evaluate:  640\n",
      "number of rows left to evaluate:  639\n",
      "number of rows left to evaluate:  638\n",
      "number of rows left to evaluate:  637\n",
      "number of rows left to evaluate:  636\n",
      "number of rows left to evaluate:  635\n",
      "number of rows left to evaluate:  634\n",
      "number of rows left to evaluate:  633\n",
      "number of rows left to evaluate:  632\n",
      "number of rows left to evaluate:  631\n",
      "number of rows left to evaluate:  630\n",
      "number of rows left to evaluate:  629\n",
      "number of rows left to evaluate:  628\n",
      "number of rows left to evaluate:  627\n",
      "number of rows left to evaluate:  626\n",
      "number of rows left to evaluate:  625\n",
      "number of rows left to evaluate:  624\n",
      "number of rows left to evaluate:  623\n",
      "number of rows left to evaluate:  622\n",
      "number of rows left to evaluate:  621\n",
      "number of rows left to evaluate:  620\n",
      "number of rows left to evaluate:  619\n",
      "number of rows left to evaluate:  618\n",
      "number of rows left to evaluate:  617\n",
      "number of rows left to evaluate:  616\n",
      "number of rows left to evaluate:  615\n",
      "number of rows left to evaluate:  614\n",
      "number of rows left to evaluate:  613\n",
      "number of rows left to evaluate:  612\n",
      "number of rows left to evaluate:  611\n",
      "number of rows left to evaluate:  610\n",
      "number of rows left to evaluate:  609\n",
      "number of rows left to evaluate:  608\n",
      "number of rows left to evaluate:  607\n",
      "number of rows left to evaluate:  606\n",
      "number of rows left to evaluate:  605\n",
      "number of rows left to evaluate:  604\n",
      "number of rows left to evaluate:  603\n",
      "number of rows left to evaluate:  602\n",
      "number of rows left to evaluate:  601\n",
      "number of rows left to evaluate:  600\n",
      "number of rows left to evaluate:  599\n",
      "number of rows left to evaluate:  598\n",
      "number of rows left to evaluate:  597\n",
      "number of rows left to evaluate:  596\n",
      "number of rows left to evaluate:  595\n",
      "number of rows left to evaluate:  594\n",
      "number of rows left to evaluate:  593\n",
      "number of rows left to evaluate:  592\n",
      "number of rows left to evaluate:  591\n",
      "number of rows left to evaluate:  590\n",
      "number of rows left to evaluate:  589\n",
      "number of rows left to evaluate:  588\n",
      "number of rows left to evaluate:  587\n",
      "number of rows left to evaluate:  586\n",
      "number of rows left to evaluate:  585\n",
      "number of rows left to evaluate:  584\n",
      "number of rows left to evaluate:  583\n",
      "number of rows left to evaluate:  582\n",
      "number of rows left to evaluate:  581\n",
      "number of rows left to evaluate:  580\n",
      "number of rows left to evaluate:  579\n",
      "number of rows left to evaluate:  578\n",
      "number of rows left to evaluate:  577\n",
      "number of rows left to evaluate:  576\n",
      "number of rows left to evaluate:  575\n",
      "number of rows left to evaluate:  574\n",
      "number of rows left to evaluate:  573\n",
      "number of rows left to evaluate:  572\n",
      "number of rows left to evaluate:  571\n",
      "number of rows left to evaluate:  570\n",
      "number of rows left to evaluate:  569\n",
      "number of rows left to evaluate:  568\n",
      "number of rows left to evaluate:  567\n",
      "number of rows left to evaluate:  566\n",
      "number of rows left to evaluate:  565\n",
      "number of rows left to evaluate:  564\n",
      "number of rows left to evaluate:  563\n",
      "number of rows left to evaluate:  562\n",
      "number of rows left to evaluate:  561\n",
      "number of rows left to evaluate:  560\n",
      "number of rows left to evaluate:  559\n",
      "number of rows left to evaluate:  558\n",
      "number of rows left to evaluate:  557\n",
      "number of rows left to evaluate:  556\n",
      "number of rows left to evaluate:  555\n",
      "number of rows left to evaluate:  554\n",
      "number of rows left to evaluate:  553\n",
      "number of rows left to evaluate:  552\n",
      "number of rows left to evaluate:  551\n",
      "number of rows left to evaluate:  550\n",
      "number of rows left to evaluate:  549\n",
      "number of rows left to evaluate:  548\n",
      "number of rows left to evaluate:  547\n",
      "number of rows left to evaluate:  546\n",
      "number of rows left to evaluate:  545\n",
      "number of rows left to evaluate:  544\n",
      "number of rows left to evaluate:  543\n",
      "number of rows left to evaluate:  542\n",
      "number of rows left to evaluate:  541\n",
      "number of rows left to evaluate:  540\n",
      "number of rows left to evaluate:  539\n",
      "number of rows left to evaluate:  538\n",
      "number of rows left to evaluate:  537\n",
      "number of rows left to evaluate:  536\n",
      "number of rows left to evaluate:  535\n",
      "number of rows left to evaluate:  534\n",
      "number of rows left to evaluate:  533\n",
      "number of rows left to evaluate:  532\n",
      "number of rows left to evaluate:  531\n",
      "number of rows left to evaluate:  530\n",
      "number of rows left to evaluate:  529\n",
      "number of rows left to evaluate:  528\n",
      "number of rows left to evaluate:  527\n",
      "number of rows left to evaluate:  526\n",
      "number of rows left to evaluate:  525\n",
      "number of rows left to evaluate:  524\n",
      "number of rows left to evaluate:  523\n",
      "number of rows left to evaluate:  522\n",
      "number of rows left to evaluate:  521\n",
      "number of rows left to evaluate:  520\n",
      "number of rows left to evaluate:  519\n",
      "number of rows left to evaluate:  518\n",
      "number of rows left to evaluate:  517\n",
      "number of rows left to evaluate:  516\n",
      "number of rows left to evaluate:  515\n",
      "number of rows left to evaluate:  514\n",
      "number of rows left to evaluate:  513\n",
      "number of rows left to evaluate:  512\n",
      "number of rows left to evaluate:  511\n",
      "number of rows left to evaluate:  510\n",
      "number of rows left to evaluate:  509\n",
      "number of rows left to evaluate:  508\n",
      "number of rows left to evaluate:  507\n",
      "number of rows left to evaluate:  506\n",
      "number of rows left to evaluate:  505\n",
      "number of rows left to evaluate:  504\n",
      "number of rows left to evaluate:  503\n",
      "number of rows left to evaluate:  502\n",
      "number of rows left to evaluate:  501\n",
      "number of rows left to evaluate:  500\n",
      "number of rows left to evaluate:  499\n",
      "number of rows left to evaluate:  498\n",
      "number of rows left to evaluate:  497\n",
      "number of rows left to evaluate:  496\n",
      "number of rows left to evaluate:  495\n",
      "number of rows left to evaluate:  494\n",
      "number of rows left to evaluate:  493\n",
      "number of rows left to evaluate:  492\n",
      "number of rows left to evaluate:  491\n",
      "number of rows left to evaluate:  490\n",
      "number of rows left to evaluate:  489\n",
      "number of rows left to evaluate:  488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-91cf00e24baf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Calculate the strategy score of LOO validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#NOTE: this might take a while due to a lack of proficiency in my programming skills, sorry ):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-606d6540140d>\u001b[0m in \u001b[0;36mloo\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_duplicate2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                         \u001b[0mcheck_duplicate2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                         \u001b[0mcount_ct2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrade\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                         \u001b[0mprobabilities2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrade\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_ct2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mgrade_dict2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrade\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-606d6540140d>\u001b[0m in \u001b[0;36mcount\u001b[1;34m(data, colname, value, target)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Grade'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2969\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3023\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3024\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3025\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3027\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3603\u001b[0m         new_data = self._data.take(\n\u001b[1;32m-> 3604\u001b[1;33m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3605\u001b[0m         )\n\u001b[0;32m   3606\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1395\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1396\u001b[0m         return self.reindex_indexer(\n\u001b[1;32m-> 1397\u001b[1;33m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1398\u001b[0m         )\n\u001b[0;32m   1399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1270\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[0mnew_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_or_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_blknos_and_blklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_rebuild_blknos_and_blklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mnew_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m             \u001b[0mnew_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_blknos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Q3b (PROGRAMMING PART)\n",
    "#------------------\n",
    "#Calculate the strategy score of LOO validation\n",
    "#NOTE: this might take a while due to less than refined implementation\n",
    "loo(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat our validation process with a shuffled dataset, expected value should be approx. 35.75%, same as above\n",
    "shuffled = shuffle(dataset)\n",
    "loo(shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy score obtained in this classifier is approximately 35.75%, which is slightly higher than the accuracy obtained in Q1, this disparity may be due to overfitting. \n",
    "- In real life, all data often contains some level of noise (outliers). \n",
    "- The implication is that the data split under holdout validation may be biased as it may not guarantee complete randomness in training and testing set, causing noise to be present in the training set. \n",
    "- When the model is trained using the flawed training set, this can lead to our model memorising the training examples, rather than trying to capture the underlying pattern / relationship between features and classes from the training examples.\n",
    "- The model may incorporate the noise in the training process, which causes our model to fit the data too well that it does not generalise to account for other samples that are not included in the training stage. \n",
    "- LOO validation helps to assess how much of an overfit our model has. \n",
    "- By iteratively holding one observation and training the model from scratch with the remaining observations, LOO validation minimises bias by maximising the training set, giving a better approximation of the true model accuracy, which lines up with our hypothesis above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risks in Student Success Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Fairness\n",
    "\n",
    "- Using machine learning in admissions process can create “self-fulling prophecies”, stereotyping certain demographic of people with certain opportunities, which encourages normalcy and deters non-conforming cases. \n",
    "- For example, our trained model may deduce a high performing student (A+, A) may be typically characterised by a specific set of feature values. \n",
    "- From our dataset, it may conclude that a stellar student would have long study hours, low number of school absences and failure rates among other features. \n",
    "- As such, the model observes these features are common in good students and incorporates them in its predictions. \n",
    "- However, these features are not reliable all the time. \n",
    "- In real life, dataset often contains outliers, noises which may undermine the distribution. \n",
    "- One example would be students that score bad grades even after studying for long hours, and vice versa. \n",
    "- Their capabilities of producing good results are unobserved because of how predictions are used to allocate grades. \n",
    "- This serves to reinforce the patterns, or “recipe” for predicting good grades that is in place. \n",
    "- The black box nature of models makes it difficult to interpret predictions. \n",
    "- Unlike humans we can justify decisions with reasons, but with models, the contribution and significant attributes to the final predicted label is not immediately visible, which can be made further obscure by the inclusion of arbitrary features. \n",
    "- Take our dataset for instance, features that are seemingly irrelevant (such as being in a romantic relationship, going out with friends) are used to predict student grades, which can affect the prediction result of our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrelevant features\n",
    "\n",
    "Let's remove some questionable features from this dataset and see how it affects our model performance. \n",
    "\n",
    "The features removed are: sex, reason to choose this school, travel time, family size, and the students' guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the questionable columns,'sex', 'reason', 'famsize', 'guardian', 'traveltime'\n",
    "small_dataset = dataset.drop(['sex', 'reason', 'famsize', 'guardian', 'traveltime'], axis = 1) \n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(small_dataset, 0.2)\n",
    "probabilities, prior_list = train(X_train, y_train)\n",
    "predictions = predict(X_test, prior_list, probabilities)\n",
    "accuracy = evaluate(y_test, predictions)\n",
    "print('Accuracy using holdout validation: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "We can see that the model performance using the tweaked dataset has improved compared to the full classifier, with an accuracy of 35.54%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat\n",
    "Despite our efforts in deleting questionable features, there is no guarantee that the resulting classifier will be fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reason 1:\n",
    "\n",
    "- One possible potential cause is a skewed dataset. To obtain our dataset, collation of data must first be conducted. \n",
    "- If any bias happens at this part, this can lead to a dataset that may not be an accurate representation of the underlying distribution. \n",
    "- Future observations will confirm predictions made by the skewed trained model more often than contradict them, compounding the bias. \n",
    "- Taking our the ‘school’ feature in dataset for example, MS may have a stricter marking policy than GP, making it harder to pass subjects. \n",
    "- Our model may fail to account for this grade inflation, and thus record that MS has a higher failure rate. \n",
    "- The model trained using the data collected hence makes predictions that are positively biased towards GP. \n",
    "\n",
    "#### Reason 2:\n",
    "\n",
    "- Another cause is there may be proxy data present. For example, even if problematic features (such as sex, reason to choose this school, household income) is removed, \n",
    "- there may exist other features that are proxies to the former (e.g. Address may be related household income). \n",
    "- The inclusion of such features will allow bias to persist in our models, and this makes feature selection a tricky process as it is difficult to determine the relationship between features in dataset and if we should include it in training.\n",
    "- Unfairness in a model can also stem from human bias in existing older data. \n",
    "- For example, if a teacher emphasizes the study time of students when assigning grades, rather than taking a holistic view of a students’ capability, a model that is trained on such grading scheme will replicate the grader's bias existing in the decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
